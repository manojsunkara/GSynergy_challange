{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9XzC-IQgvUA",
        "outputId": "4ad467c8-78bd-42a8-bae0-5c41638a4c19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extraction complete!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Path where files are uploaded\n",
        "data_path = \"/content/sample_data/data\"\n",
        "\n",
        "# Extract all .gz files\n",
        "for file in os.listdir(data_path):\n",
        "    if file.endswith(\".gz\"):\n",
        "        os.system(f\"gunzip {os.path.join(data_path, file)}\")\n",
        "\n",
        "print(\"Extraction complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dP_I4e-whY7_",
        "outputId": "d1c44850-cae8-4637-9e33-28767b449d79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfbvlmHZhcq-",
        "outputId": "e02bd07b-29c2-4b2b-d658-e058fecd386f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark session initialized!\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"ETL_Pipeline\").getOrCreate()\n",
        "\n",
        "print(\"Spark session initialized!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLa8OlevhfsA",
        "outputId": "d554e409-2e1e-4119-c508-64c506ab9304"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+-------+----+-------------------+-----------+----------+---------+-----------------+-----------+-------------+----------------+-----------------+----------------+\n",
            "| order_id|line_id|type|                 dt|pos_site_id|    sku_id|fscldt_id|price_substate_id|sales_units|sales_dollars|discount_dollars|original_order_id|original_line_id|\n",
            "+---------+-------+----+-------------------+-----------+----------+---------+-----------------+-----------+-------------+----------------+-----------------+----------------+\n",
            "|164087401|      2|Sale|2016-01-31 06:17:01|    CATMAIN|2668940801| 20160131|               FP|          1|        58.95|             0.0|             NULL|            NULL|\n",
            "|164087409|      4|Sale|2016-01-31 06:17:25|    CATMAIN|2920920601| 20160131|               FP|          1|        49.95|             0.0|             NULL|            NULL|\n",
            "|164087440|      2|Sale|2016-01-31 06:19:28|   INETMAIN|0695690000| 20160131|               FP|          2|         37.9|             0.0|             NULL|            NULL|\n",
            "|164087469|      2|Sale|2016-01-31 07:03:00|    CATMAIN|0505490000| 20160131|               FP|          1|         45.0|             0.0|             NULL|            NULL|\n",
            "|164087472|      2|Sale|2016-01-31 07:07:36|    CATMAIN|2872180801| 20160131|               FP|          1|        98.95|             0.0|             NULL|            NULL|\n",
            "+---------+-------+----+-------------------+-----------+----------+---------+-----------------+-----------+-------------+----------------+-----------------+----------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+----------+--------------------+----------+--------------------+-------+--------------------+---------+------------+------+---------+-------+----------+-----+--------+-----+\n",
            "|    sku_id|           sku_label|stylclr_id|       stylclr_label|styl_id|          styl_label|subcat_id|subcat_label|cat_id|cat_label|dept_id|dept_label|issvc|isasmbly|isnfs|\n",
            "+----------+--------------------+----------+--------------------+-------+--------------------+---------+------------+------+---------+-------+----------+-----+--------+-----+\n",
            "|2230940206|COTTON UNDERWIRE ...|   2230940|COTTON UNDERWIRE ...|  22309|COTTON UNDERWIRE ...|      405|        TOPS|  1000|     TOPS|   2000|   APPAREL|    0|       0|    0|\n",
            "|2230940208|COTTON UNDERWIRE ...|   2230940|COTTON UNDERWIRE ...|  22309|COTTON UNDERWIRE ...|      405|        TOPS|  1000|     TOPS|   2000|   APPAREL|    0|       0|    0|\n",
            "|2230940308|COTTON UNDERWIRE ...|   2230940|COTTON UNDERWIRE ...|  22309|COTTON UNDERWIRE ...|      405|        TOPS|  1000|     TOPS|   2000|   APPAREL|    0|       0|    0|\n",
            "|2230980302|COTTON UNDERWIRE ...|   2230980|COTTON UNDERWIRE ...|  22309|COTTON UNDERWIRE ...|      405|        TOPS|  1000|     TOPS|   2000|   APPAREL|    0|       0|    0|\n",
            "|2231033324|UNDERWIRE 3/4 SLE...|   2231033|UNDERWIRE 3/4 SLE...|  22310|UNDERWIRE 3/4 SLE...|      405|        TOPS|  1000|     TOPS|   2000|   APPAREL|    0|       0|    0|\n",
            "+----------+--------------------+----------+--------------------+-------+--------------------+---------+------------+------+---------+-------+----------+-----+--------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Define data path\n",
        "data_path = \"/content/sample_data/data\"\n",
        "\n",
        "# Load transactions data\n",
        "df_transactions = spark.read.option(\"delimiter\", \"|\").csv(f\"{data_path}/fact.transactions.dlm\", header=True, inferSchema=True)\n",
        "\n",
        "# Load product hierarchy data\n",
        "df_products = spark.read.option(\"delimiter\", \"|\").csv(f\"{data_path}/hier.prod.dlm\", header=True, inferSchema=True)\n",
        "\n",
        "# Show first few rows\n",
        "df_transactions.show(5)\n",
        "df_products.show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BOjXap2hksr",
        "outputId": "0b00574b-b1dd-4832-dbc6-a86488e0c943"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+----------+---------+-----------------+-------------------+----------------------+\n",
            "|pos_site_id|    sku_id|fscldt_id|total_sales_units|total_sales_dollars|total_discount_dollars|\n",
            "+-----------+----------+---------+-----------------+-------------------+----------------------+\n",
            "|    CATMAIN|2598481801| 20160131|                1|              59.95|                   0.0|\n",
            "|    CATMAIN|0695890000| 20160201|                5|             374.75|                   0.0|\n",
            "|    CATMAIN|6831941800| 20160201|               10|              349.5|                   0.0|\n",
            "|    CATMAIN|0174410000| 20160203|               14|             247.11|                 57.39|\n",
            "|    CATMAIN|0487612000| 20160204|               23|             563.75|                 11.25|\n",
            "|    CATMAIN|2666820701| 20160204|                1|              49.95|                   0.0|\n",
            "|    CATMAIN|2870720701| 20160205|                7|             454.65|                   0.0|\n",
            "|    CATMAIN|5844840913| 20160206|                2|              189.9|                   0.0|\n",
            "|    CATMAIN|4840520704| 20160208|               11|  741.4799999999999|                 27.97|\n",
            "|        146|2668941701| 20160208|                1|              58.95|                   0.0|\n",
            "+-----------+----------+---------+-----------------+-------------------+----------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import sum\n",
        "\n",
        "# Join transactions with product hierarchy on 'sku_id' (ensure this column exists in both tables)\n",
        "df_joined = df_transactions.join(df_products, \"sku_id\", \"left\")\n",
        "\n",
        "# Use fscldt_id (fiscal date ID) instead of fsclwk_id\n",
        "df_summary = df_joined.groupBy(\"pos_site_id\", \"sku_id\", \"fscldt_id\").agg(\n",
        "    sum(\"sales_units\").alias(\"total_sales_units\"),\n",
        "    sum(\"sales_dollars\").alias(\"total_sales_dollars\"),\n",
        "    sum(\"discount_dollars\").alias(\"total_discount_dollars\")\n",
        ")\n",
        "\n",
        "df_summary.show(10)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOxmOkoX-4NQ",
        "outputId": "bc5fcecf-2e51-449b-f64c-aa6ebf7d323e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔹 Extraction complete!\n",
            "✅ Incremental update applied!\n",
            "✅ Data successfully written to /content/sample_data/output/mview_weekly_sales\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import sum, col\n",
        "\n",
        "# Step 1: Initialize Spark Session\n",
        "spark = SparkSession.builder.appName(\"RetailDataPipeline\").getOrCreate()\n",
        "\n",
        "# Define input and output paths\n",
        "input_path = \"/content/sample_data/data\"\n",
        "output_path = \"/content/sample_data/output/mview_weekly_sales\"\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "# Step 2: Extract .gz files if present\n",
        "for file in os.listdir(input_path):\n",
        "    if file.endswith(\".gz\"):\n",
        "        os.system(f\"gunzip {os.path.join(input_path, file)}\")\n",
        "\n",
        "print(\"🔹 Extraction complete!\")\n",
        "\n",
        "# Step 3: Load Fact Transactions Data\n",
        "df_transactions = spark.read.option(\"delimiter\", \"|\").csv(\n",
        "    f\"{input_path}/fact.transactions.dlm\", header=True, inferSchema=True\n",
        ")\n",
        "\n",
        "# Step 4: Load Product Hierarchy (Dimension) Data\n",
        "df_products = spark.read.option(\"delimiter\", \"|\").csv(\n",
        "    f\"{input_path}/hier.prod.dlm\", header=True, inferSchema=True\n",
        ")\n",
        "\n",
        "# Step 5: Data Cleaning & Validation\n",
        "df_transactions = df_transactions.dropna(\n",
        "    subset=[\"order_id\", \"line_id\", \"sku_id\", \"pos_site_id\", \"fscldt_id\"]\n",
        ")\n",
        "df_products = df_products.dropna(subset=[\"sku_id\"])\n",
        "\n",
        "# Check for valid Foreign Key Constraint (sku_id in both tables)\n",
        "invalid_fk_count = df_transactions.join(df_products, \"sku_id\", \"left_anti\").count()\n",
        "if invalid_fk_count > 0:\n",
        "    print(f\"⚠️ Warning: {invalid_fk_count} transactions have invalid sku_id!\")\n",
        "\n",
        "# Step 6: Join transactions with product hierarchy on 'sku_id'\n",
        "df_joined = df_transactions.join(df_products, \"sku_id\", \"left\")\n",
        "\n",
        "# Step 7: Aggregate Data by pos_site_id, sku_id, fscldt_id\n",
        "df_summary = df_joined.groupBy(\"pos_site_id\", \"sku_id\", \"fscldt_id\").agg(\n",
        "    sum(\"sales_units\").alias(\"total_sales_units\"),\n",
        "    sum(\"sales_dollars\").alias(\"total_sales_dollars\"),\n",
        "    sum(\"discount_dollars\").alias(\"total_discount_dollars\")\n",
        ")\n",
        "\n",
        "# ✅ **Step 8: Handle Incremental Updates**\n",
        "try:\n",
        "    # Load existing mview_weekly_sales if it exists\n",
        "    df_existing = spark.read.option(\"header\", \"true\").csv(output_path)\n",
        "\n",
        "    # Convert columns to correct types\n",
        "    df_existing = df_existing.withColumn(\"total_sales_units\", col(\"total_sales_units\").cast(\"double\"))\n",
        "    df_existing = df_existing.withColumn(\"total_sales_dollars\", col(\"total_sales_dollars\").cast(\"double\"))\n",
        "    df_existing = df_existing.withColumn(\"total_discount_dollars\", col(\"total_discount_dollars\").cast(\"double\"))\n",
        "\n",
        "    # Merge new summary with existing data (incremental logic)\n",
        "    df_final = df_existing.union(df_summary).groupBy(\"pos_site_id\", \"sku_id\", \"fscldt_id\").agg(\n",
        "        sum(\"total_sales_units\").alias(\"total_sales_units\"),\n",
        "        sum(\"total_sales_dollars\").alias(\"total_sales_dollars\"),\n",
        "        sum(\"total_discount_dollars\").alias(\"total_discount_dollars\")\n",
        "    )\n",
        "\n",
        "    print(\"✅ Incremental update applied!\")\n",
        "\n",
        "except Exception as e:\n",
        "    # If no previous data exists, use the new summary directly\n",
        "    print(\"⚠️ No existing mview_weekly_sales found. Creating new table.\")\n",
        "    df_final = df_summary\n",
        "\n",
        "# Step 9: Save Final Data\n",
        "df_final.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(output_path)\n",
        "\n",
        "print(f\"✅ Data successfully written to {output_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Bkk1sV-_5mB",
        "outputId": "6dbe71a1-9ab8-4770-a6f9-b26eacf207bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔹 Extraction complete!\n",
            "✅ Incremental update applied!\n",
            "✅ Data successfully written to /content/sample_data/output/mview_weekly_sales\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import sum, col\n",
        "\n",
        "# Step 1: Initialize Spark Session\n",
        "spark = SparkSession.builder.appName(\"RetailDataPipeline\").getOrCreate()\n",
        "\n",
        "# Enable checkpointing (Partial Checkpoint)\n",
        "checkpoint_dir = \"/content/sample_data/checkpoints\"\n",
        "spark.sparkContext.setCheckpointDir(checkpoint_dir)\n",
        "\n",
        "# Define input and output paths\n",
        "input_path = \"/content/sample_data/data\"\n",
        "output_path = \"/content/sample_data/output/mview_weekly_sales\"\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "# Step 2: Extract .gz files if present\n",
        "for file in os.listdir(input_path):\n",
        "    if file.endswith(\".gz\"):\n",
        "        os.system(f\"gunzip {os.path.join(input_path, file)}\")\n",
        "\n",
        "print(\"🔹 Extraction complete!\")\n",
        "\n",
        "# Step 3: Load Fact Transactions Data\n",
        "df_transactions = spark.read.option(\"delimiter\", \"|\").csv(\n",
        "    f\"{input_path}/fact.transactions.dlm\", header=True, inferSchema=True\n",
        ")\n",
        "\n",
        "# Step 4: Load Product Hierarchy (Dimension) Data\n",
        "df_products = spark.read.option(\"delimiter\", \"|\").csv(\n",
        "    f\"{input_path}/hier.prod.dlm\", header=True, inferSchema=True\n",
        ")\n",
        "\n",
        "# Step 5: Data Cleaning & Validation\n",
        "df_transactions = df_transactions.dropna(\n",
        "    subset=[\"order_id\", \"line_id\", \"sku_id\", \"pos_site_id\", \"fscldt_id\"]\n",
        ")\n",
        "df_products = df_products.dropna(subset=[\"sku_id\"])\n",
        "\n",
        "# Check for valid Foreign Key Constraint (sku_id in both tables)\n",
        "invalid_fk_count = df_transactions.join(df_products, \"sku_id\", \"left_anti\").count()\n",
        "if invalid_fk_count > 0:\n",
        "    print(f\"⚠️ Warning: {invalid_fk_count} transactions have invalid sku_id!\")\n",
        "\n",
        "# Step 6: Join transactions with product hierarchy on 'sku_id'\n",
        "df_joined = df_transactions.join(df_products, \"sku_id\", \"left\")\n",
        "\n",
        "# ✅ **Apply Partial Checkpoint after Join (to optimize performance)**\n",
        "df_joined = df_joined.checkpoint(eager=True)\n",
        "\n",
        "# Step 7: Aggregate Data by pos_site_id, sku_id, fscldt_id\n",
        "df_summary = df_joined.groupBy(\"pos_site_id\", \"sku_id\", \"fscldt_id\").agg(\n",
        "    sum(\"sales_units\").alias(\"total_sales_units\"),\n",
        "    sum(\"sales_dollars\").alias(\"total_sales_dollars\"),\n",
        "    sum(\"discount_dollars\").alias(\"total_discount_dollars\")\n",
        ")\n",
        "\n",
        "# ✅ **Apply Partial Checkpoint after Aggregation (to avoid recomputation)**\n",
        "df_summary = df_summary.checkpoint(eager=True)\n",
        "\n",
        "# ✅ **Step 8: Handle Incremental Updates with Deduplication**\n",
        "try:\n",
        "    # Load existing data if it exists\n",
        "    df_existing = spark.read.option(\"header\", \"true\").csv(output_path)\n",
        "\n",
        "    # Convert columns to correct types\n",
        "    df_existing = df_existing.withColumn(\"total_sales_units\", col(\"total_sales_units\").cast(\"double\"))\n",
        "    df_existing = df_existing.withColumn(\"total_sales_dollars\", col(\"total_sales_dollars\").cast(\"double\"))\n",
        "    df_existing = df_existing.withColumn(\"total_discount_dollars\", col(\"total_discount_dollars\").cast(\"double\"))\n",
        "\n",
        "    # Merge new summary with existing data, ensuring no duplicates\n",
        "    df_final = df_existing.union(df_summary).groupBy(\"pos_site_id\", \"sku_id\", \"fscldt_id\").agg(\n",
        "        sum(\"total_sales_units\").alias(\"total_sales_units\"),\n",
        "        sum(\"total_sales_dollars\").alias(\"total_sales_dollars\"),\n",
        "        sum(\"total_discount_dollars\").alias(\"total_discount_dollars\")\n",
        "    )\n",
        "\n",
        "    # Remove exact duplicates\n",
        "    df_final = df_final.dropDuplicates([\"pos_site_id\", \"sku_id\", \"fscldt_id\"])\n",
        "\n",
        "    print(\"✅ Incremental update applied!\")\n",
        "\n",
        "except Exception as e:\n",
        "    # If no previous data exists, use the new summary directly\n",
        "    print(\"⚠️ No existing mview_weekly_sales found. Creating new table.\")\n",
        "    df_final = df_summary\n",
        "\n",
        "# ✅ **Apply Partial Checkpoint before Writing (Final Optimization)**\n",
        "df_final = df_final.checkpoint(eager=True)\n",
        "\n",
        "# Step 9: Save Final Data in Append Mode\n",
        "df_final.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(output_path)\n",
        "\n",
        "print(f\"✅ Data successfully written to {output_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Final Code"
      ],
      "metadata": {
        "id": "74cCX0xwjAuK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbVb2i4ADm9H",
        "outputId": "70a4d214-5da6-431f-af7f-925edabd5d97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⚠️ No Existing Data Found. Creating Fresh Dataset.\n",
            "✅ Data processing complete! Output saved at: /content/sample_data/output/mview_weekly_sales\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, sum\n",
        "import os\n",
        "\n",
        "# Step 1: Initialize Spark Session\n",
        "spark = SparkSession.builder.appName(\"RetailDataProcessor\").getOrCreate()\n",
        "\n",
        "# Define input and output paths\n",
        "input_dir = \"/content/sample_data/data\"\n",
        "output_dir = \"/content/sample_data/output/mview_weekly_sales\"\n",
        "\n",
        "# Step 2: Extract .gz files\n",
        "for filename in os.listdir(input_dir):\n",
        "    if filename.endswith(\".gz\"):\n",
        "        os.system(f\"gunzip {os.path.join(input_dir, filename)}\")\n",
        "\n",
        "# Step 3: Load data into Spark DataFrames\n",
        "transactions_df = spark.read.option(\"delimiter\", \"|\").csv(\n",
        "    f\"{input_dir}/fact.transactions.dlm\", header=True, inferSchema=True\n",
        ")\n",
        "\n",
        "products_df = spark.read.option(\"delimiter\", \"|\").csv(\n",
        "    f\"{input_dir}/hier.prod.dlm\", header=True, inferSchema=True\n",
        ")\n",
        "\n",
        "# Step 4: Clean Data - Remove Null Values\n",
        "transactions_df = transactions_df.dropna(subset=[\"order_id\", \"line_id\", \"sku_id\", \"pos_site_id\", \"fscldt_id\"])\n",
        "products_df = products_df.dropna(subset=[\"sku_id\"])\n",
        "\n",
        "# Step 5: Foreign Key Validation (Ensuring sku_id exists in both tables)\n",
        "missing_sku_count = transactions_df.join(products_df, \"sku_id\", \"left_anti\").count()\n",
        "if missing_sku_count > 0:\n",
        "    print(f\"⚠️ Warning: {missing_sku_count} records have invalid SKU IDs!\")\n",
        "\n",
        "# Step 6: Perform Join to enrich transaction data\n",
        "merged_df = transactions_df.join(products_df, \"sku_id\", \"left\")\n",
        "\n",
        "# Apply Partial Checkpointing (To Optimize Spark Processing)\n",
        "merged_df = merged_df.checkpoint(eager=True)\n",
        "\n",
        "# Step 7: Aggregate Sales Data\n",
        "summary_df = merged_df.groupBy(\"pos_site_id\", \"sku_id\", \"fscldt_id\").agg(\n",
        "    sum(\"sales_units\").alias(\"total_units\"),\n",
        "    sum(\"sales_dollars\").alias(\"total_revenue\"),\n",
        "    sum(\"discount_dollars\").alias(\"total_discounts\")\n",
        ")\n",
        "\n",
        "# Apply Checkpointing for Intermediate Data\n",
        "summary_df = summary_df.checkpoint(eager=True)\n",
        "\n",
        "# Step 8: Handle Incremental Updates\n",
        "try:\n",
        "    existing_df = spark.read.option(\"header\", \"true\").csv(output_dir)\n",
        "\n",
        "    # Convert columns to appropriate types for aggregation\n",
        "    existing_df = existing_df.withColumn(\"total_units\", col(\"total_units\").cast(\"double\"))\n",
        "    existing_df = existing_df.withColumn(\"total_revenue\", col(\"total_revenue\").cast(\"double\"))\n",
        "    existing_df = existing_df.withColumn(\"total_discounts\", col(\"total_discounts\").cast(\"double\"))\n",
        "\n",
        "    # Merge new and existing data\n",
        "    final_df = existing_df.union(summary_df).groupBy(\"pos_site_id\", \"sku_id\", \"fscldt_id\").agg(\n",
        "        sum(\"total_units\").alias(\"total_units\"),\n",
        "        sum(\"total_revenue\").alias(\"total_revenue\"),\n",
        "        sum(\"total_discounts\").alias(\"total_discounts\")\n",
        "    )\n",
        "\n",
        "    # Remove duplicate records\n",
        "    final_df = final_df.dropDuplicates([\"pos_site_id\", \"sku_id\", \"fscldt_id\"])\n",
        "    print(\"✅ Incremental Update Applied Successfully!\")\n",
        "\n",
        "except:\n",
        "    print(\"⚠️ No Existing Data Found. Creating Fresh Dataset.\")\n",
        "    final_df = summary_df\n",
        "\n",
        "# Step 9: Save Final Output\n",
        "final_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(output_dir)\n",
        "\n",
        "print(f\"✅ Data processing complete! Output saved at: {output_dir}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}